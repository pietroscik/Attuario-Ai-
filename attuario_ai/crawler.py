"""Simple crawler constrained to a single domain.\n\n"""\n\nfrom __future__ import annotations\n\nimport time\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom typing import Deque, Iterable, Optional, Set\nfrom urllib.parse import urljoin, urlparse\nfrom urllib import robotparser\n\nimport requests\nfrom requests import Response\n\n@dataclass\nclass CrawlResult:\n    """Represents a crawled page."""\n    url: str\n    status_code: int\n    html: str\n    fetched_at: float\n    referer: Optional[str] = None\n    error: Optional[str] = None\n\nclass RobotsPolicy:\n    """Utility wrapper around ``robots.txt`` directives for the target domain."""\n    def __init__(self, base_url: str, *, session: requests.Session, timeout: float, user_agent: str) -> None:\n        self.user_agent = user_agent\n        self._parser = robotparser.RobotFileParser()\n        self._available = False\n        self.crawl_delay: Optional[float] = None\n        self.sitemaps: tuple[str, ...] = ()\n\n        robots_url = urljoin(base_url.rstrip("/"), "/robots.txt")\n        try:\n            response = session.get(robots_url, timeout=timeout)\n            if response.status_code < 400 and response.text.strip():\n                self._parser.parse(response.text.splitlines())\n                self._available = True\n                self.crawl_delay = self._parser.crawl_delay(user_agent) or self._parser.crawl_delay("*")\n                sitemaps = self._parser.site_maps() or []\n                self.sitemaps = tuple(sitemaps)\n        except requests.RequestException:\n            self._available = False\n\n    def allows(self, url: str) -> bool:\n        if not self._available:\n            return True\n        return self._parser.can_fetch(self.user_agent, url)\n\nclass Crawler:\n    """Breadth-first crawler restricted to a target domain.\n\n    The crawler respects ``robots.txt`` directives for the target domain."""\n    def __init__(self, base_url: str, *, max_pages: int = 100, max_depth: int = 2, delay_seconds: float = 0.5, session: Optional[requests.Session] = None, timeout: float = 10.0, user_agent: str = "AttuarioAI/0.1 (+https://github.com)") -> None:\n        self.base_url = base_url.rstrip("/")\n        self.max_pages = max_pages\n        self.max_depth = max_depth\n        self.delay_seconds = delay_seconds\n        self.timeout = timeout\n        self._session = session or requests.Session()\n        self._session.headers.setdefault("User-Agent", user_agent)\n\n        parsed = urlparse(self.base_url)\n        if not parsed.scheme or not parsed.netloc:\n            raise ValueError(f"Invalid base_url: {base_url}")\n        self._netloc = parsed.netloc\n\n        self._robots = RobotsPolicy(self.base_url, session=self._session, timeout=self.timeout, user_agent=self._session.headers["User-Agent"])\n        if self._robots.crawl_delay:\n            self.delay_seconds = max(self.delay_seconds, self._robots.crawl_delay)\n\n    def close(self) -> None:\n        self._session.close()\n\n    def crawl(self, seeds: Optional[Iterable[str]] = None) -> Iterable[CrawlResult]:\n        queue: Deque[tuple[str, int, Optional[str]]] = deque()\n        visited: Set[str] = set()\n\n        if seeds is None:\n            queue.append((self.base_url, 0, None))\n        else:\n            for seed in seeds:\n                queue.append((seed, 0, None))\n\n        pages_crawled = 0\n\n        while queue and pages_crawled < self.max_pages:\n            url, depth, referer = queue.popleft()\n            normalized = self._normalize_url(url)\n            if normalized in visited:\n                continue\n            if not self._robots.allows(normalized):\n                continue\n            visited.add(normalized)\n\n            result = self._fetch(normalized, referer)\n            yield result\n            pages_crawled += 1\n\n            if result.error or depth >= self.max_depth:\n                continue\n\n            for link in self._extract_links(result.html, normalized):\n                if link not in visited and self._robots.allows(link):\n                    queue.append((link, depth + 1, normalized))\n\n            if queue and self.delay_seconds > 0:\n                time.sleep(self.delay_seconds)\n\n    def _fetch(self, url: str, referer: Optional[str]) -> CrawlResult:\n        try:\n            response: Response = self._session.get(url, timeout=self.timeout)\n            response.raise_for_status()\n            html = response.text\n            status_code = response.status_code\n            error = None\n        except requests.RequestException as exc:\n            html = ""\n            status_code = (getattr(exc.response, "status_code", 0) if hasattr(exc, "response") else 0)\n            error = str(exc)\n        return CrawlResult(url=url, status_code=status_code, html=html, fetched_at=time.time(), referer=referer, error=error)\n\n    def _extract_links(self, html: str, current_url: str) -> Set[str]:\n        from bs4 import (BeautifulSoup,)\n\n        # lazy import to keep dependency optional for non-crawl use\n\n        soup = BeautifulSoup(html, "html.parser")\n        links: Set[str] = set()\n        for tag in soup.find_all("a", href=True):\n            href = tag["href"].strip()\n            if href.startswith("#"):\n                continue\n            joined = urljoin(current_url, href)\n            parsed = urlparse(joined)\n            if parsed.netloc == self._netloc and parsed.scheme in {"http", "https"}:\n                clean = self._normalize_url(joined)\n                links.add(clean)\n        return links\n\n    def _normalize_url(self, url: str) -> str:\n        parsed = urlparse(url)\n        normalized = parsed._replace(fragment="").geturl()\n        if normalized.endswith("/") and normalized != self.base_url:\n            normalized = normalized[:-1]\n        return normalized\n\n    def __enter__(self) -> "Crawler":\n        return self\n\n    def __exit__(self, *exc_info) -> None:\n        self.close()\n